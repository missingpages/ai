[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/FlashPost - Counting LLM tokens/2024-10-06-FlashNote-TokenCounting.html",
    "href": "posts/FlashPost - Counting LLM tokens/2024-10-06-FlashNote-TokenCounting.html",
    "title": "“FlashPost",
    "section": "",
    "text": "- To count message tokens to and from openAI LLM\"\n\n“This post tells you different ways to count the tokens, both the input tokens sent to LLM and the tokens in output response”\n\n\n1. Set env key for openai\n\nfrom langchain_openai import ChatOpenAI\n\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n\n\n\n2. Use langchain\nLets use the langchain library to chat with LLM\n\nllm = ChatOpenAI(model= \"gpt-4o-mini\", temperature=0)\n\n\naimessage = llm.invoke(\"how are you?\")\naimessage\n\nAIMessage(content=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 11, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-7482b300-97f1-48a1-a990-78a84410220c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 28, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})\n\n\n\nmessages = [{\"role\":\"user\",\"content\":\"how are you?\"}]\n\nThe above response message says ‘prompt_tokens’: 11 and output tokens as 28\nlets verify it\n\n\n3. Verify with tiktoken library\n\nimport tiktoken\n\n\nencoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n\n\nencoding.encode(\"how are you?\")\n\n[8923, 553, 481, 30]\n\n\nLets decode and verify the text\n\nencoding.decode([8923, 553, 481, 30])\n\n'how are you?'\n\n\nwe can also see how the tokens are split interactively by acccessing https://platform.openai.com/tokenizer\n\n\nAs seen above, the result above matches with the result from tiktoken library, however the count doesnot match with response AIMessage why?\nAs openAI expects the input request be sent with “role” and “content” as dictionary, langchain adds it internally even if we specify the message as plain string. we could check the details by turning on the debug mode\n\n\nimport langchain\nlangchain.debug=True\n\n\nllm.invoke(\"how are you?\")\n\n[llm/start] [llm:ChatOpenAI] Entering LLM run with input:\n{\n  \"prompts\": [\n    \"Human: how are you?\"\n  ]\n}\n[llm/end] [llm:ChatOpenAI] [1.27s] Exiting LLM run with output:\n{\n  \"generations\": [\n    [\n      {\n        \"text\": \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\",\n        \"generation_info\": {\n          \"finish_reason\": \"stop\",\n          \"logprobs\": null\n        },\n        \"type\": \"ChatGeneration\",\n        \"message\": {\n          \"lc\": 1,\n          \"type\": \"constructor\",\n          \"id\": [\n            \"langchain\",\n            \"schema\",\n            \"messages\",\n            \"AIMessage\"\n          ],\n          \"kwargs\": {\n            \"content\": \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\",\n            \"additional_kwargs\": {\n              \"refusal\": null\n            },\n            \"response_metadata\": {\n              \"token_usage\": {\n                \"completion_tokens\": 28,\n                \"prompt_tokens\": 11,\n                \"total_tokens\": 39,\n                \"completion_tokens_details\": {\n                  \"audio_tokens\": null,\n                  \"reasoning_tokens\": 0\n                },\n                \"prompt_tokens_details\": {\n                  \"audio_tokens\": null,\n                  \"cached_tokens\": 0\n                }\n              },\n              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n              \"system_fingerprint\": \"fp_f85bea6784\",\n              \"finish_reason\": \"stop\",\n              \"logprobs\": null\n            },\n            \"type\": \"ai\",\n            \"id\": \"run-a81f00b2-3c0a-4b8f-8a59-756b8740a9ec-0\",\n            \"usage_metadata\": {\n              \"input_tokens\": 11,\n              \"output_tokens\": 28,\n              \"total_tokens\": 39,\n              \"input_token_details\": {\n                \"cache_read\": 0\n              },\n              \"output_token_details\": {\n                \"reasoning\": 0\n              }\n            },\n            \"tool_calls\": [],\n            \"invalid_tool_calls\": []\n          }\n        }\n      }\n    ]\n  ],\n  \"llm_output\": {\n    \"token_usage\": {\n      \"completion_tokens\": 28,\n      \"prompt_tokens\": 11,\n      \"total_tokens\": 39,\n      \"completion_tokens_details\": {\n        \"audio_tokens\": null,\n        \"reasoning_tokens\": 0\n      },\n      \"prompt_tokens_details\": {\n        \"audio_tokens\": null,\n        \"cached_tokens\": 0\n      }\n    },\n    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n    \"system_fingerprint\": \"fp_f85bea6784\"\n  },\n  \"run\": null,\n  \"type\": \"LLMResult\"\n}\n\n\nAIMessage(content=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 11, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-a81f00b2-3c0a-4b8f-8a59-756b8740a9ec-0', usage_metadata={'input_tokens': 11, 'output_tokens': 28, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})\n\n\nNow lets check what is the token count after putting it in proper format\n\nencoding.encode('{role:user,content:how are you?}')\n\n[90, 8716, 114851, 11, 3252, 25, 8923, 553, 481, 30, 92]\n\n\nAnd what is the content token length? It matches!\n\nlen(encoding.encode(aimessage.content))\n\n28"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "missing pages",
    "section": "",
    "text": "FlashPost #1 - How to count message tokens\n\n\n\n\n\n\nllm\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nBalaji\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\nBalaji\n\n\n\n\n\n\nNo matching items"
  }
]